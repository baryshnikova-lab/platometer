import datetime
import time
import os
import argparse
import sys
import traceback
import dill as pickle

# Necessary check to make sure code runs both in Jupyter and in command line
if 'matplotlib' not in sys.modules:
    import matplotlib
    matplotlib.use('Agg')

import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as patches

import numpy as np
import pandas as pd
import multiprocessing as mp
import scipy.optimize

from scipy.signal import medfilt2d
from scipy import ndimage as ndi
from sklearn.mixture import GaussianMixture
from skimage import filters
from skimage.morphology import watershed

from platometer_io import *
from platometer_utils import *


class Platometer:

    def __init__(self, path_to_image_file, verbose=True):

        self.path_to_image_file = path_to_image_file
        self.verbose = verbose

        self.im = plt.imread(path_to_image_file)

        self.im_center = None
        self.fit_row = None
        self.fit_col = None

        self.im_gray_trimmed = None
        self.im_foreground = None
        self.im_objects = None

        # List of colony center coordinates
        self.colony_pxl = None

        # DataFrame with final output
        self.colony_data = None

    def get_path_to_output_file(self, extension=None):

        file_name = os.path.basename(self.path_to_image_file)
        parent_folder = os.path.dirname(os.path.dirname(self.path_to_image_file))
        data_folder = os.path.join(parent_folder, '02_data')

        if not os.path.exists(data_folder):
            os.makedirs(data_folder)

        if extension:
            path_to_output_file = os.path.join(data_folder, file_name + extension)
        else:
            path_to_output_file = data_folder

        return path_to_output_file

    def save(self):

        path_to_p_file = self.get_path_to_output_file('.p')

        with open(path_to_p_file, 'wb') as file_handle:
            pickle.dump(self, file_handle)

    def print(self):

        path_to_dat_file = self.get_path_to_output_file('.dat')

        self.colony_data['circularity'] = np.nan
        self.colony_data['flags'] = np.nan

        f = open(path_to_dat_file, 'w')
        f.write(format('# Data file generated by platometer on %s\n' % datetime.datetime.now()))
        f.write('# row\tcol\tsize\tcircularity\tflags\n')
        self.colony_data.to_csv(f, columns=['row', 'col', 'size', 'circularity', 'flags'],
                                sep='\t', header=False, index=False)
        f.close()

    def gray_and_trim(self):

        # Convert to gray scale
        im_gray = np.nanmean(self.im, axis=2)

        # Downsample 2X
        im_gray = bucket(im_gray, [2, 2])

        # Medial filter
        im_gray = medfilt2d(im_gray, kernel_size=5)

        # Get row & col averages
        row_avg = np.nanmean(im_gray, axis=1)
        col_avg = np.nanmean(im_gray, axis=0)

        # Find the positions of colony centers by fitting a sinusoid to the row and col averages
        self.fit_row = fit_sin(np.arange(len(row_avg)), row_avg, guess_freq=1/45)
        self.fit_col = fit_sin(np.arange(len(col_avg)), col_avg, guess_freq=1/45)

        row_pxl = np.arange(len(row_avg))
        col_pxl = np.arange(len(col_avg))

        row_avg_fit = self.fit_row['fitfunc'](row_pxl)
        col_avg_fit = self.fit_col['fitfunc'](col_pxl)

        # Define the expected size of a window containing 32 rows or 48 columns
        w_row = np.ceil(self.fit_row['period'] * 32).astype(int)
        w_col = np.ceil(self.fit_col['period'] * 48).astype(int)

        # Find the boundaries of the plate (when plate is not centered within the image)
        plate_row_boundaries = [np.min(np.nonzero(row_avg > 100)), np.max(np.nonzero(row_avg > 100))]
        plate_col_boundaries = [np.min(np.nonzero(col_avg > 100)), np.max(np.nonzero(col_avg > 100))]

        # Find the approximate middle of the plate
        best_w_row_mid_pxl = int(np.min(plate_row_boundaries) + np.abs(np.diff(plate_row_boundaries) / 2))
        best_w_col_mid_pxl = int(np.min(plate_col_boundaries) + np.abs(np.diff(plate_col_boundaries) / 2))

        # Snap to the closest negative peak of the fitted sinusoid
        row_negative_peaks_px = detect_peaks(row_avg_fit, mpd=self.fit_row['period'] / 2,
                                             edge='both', kpsh=True, valley=True, show=False)
        col_negative_peaks_px = detect_peaks(col_avg_fit, mpd=self.fit_col['period'] / 2,
                                             edge='both', kpsh=True, valley=True, show=False)

        best_w_row_mid_pxl = row_negative_peaks_px[np.argmin(np.abs(best_w_row_mid_pxl - row_negative_peaks_px))]
        best_w_col_mid_pxl = col_negative_peaks_px[np.argmin(np.abs(best_w_col_mid_pxl - col_negative_peaks_px))]

        trim_pxl_row = [best_w_row_mid_pxl - w_row / 2 - self.fit_row['period'] / 4,
                        best_w_row_mid_pxl + w_row / 2 + self.fit_row['period'] / 4]
        trim_pxl_col = [best_w_col_mid_pxl - w_col / 2 - self.fit_col['period'] / 4,
                        best_w_col_mid_pxl + w_col / 2 + self.fit_col['period'] / 4]

        # Trim the image around the best window
        im_gray_trimmed = im_gray.copy()

        row_mask = row_pxl[(row_pxl > trim_pxl_row[0]) & (row_pxl < trim_pxl_row[1])]
        col_mask = col_pxl[(col_pxl > trim_pxl_col[0]) & (col_pxl < trim_pxl_col[1])]
        im_gray_trimmed = im_gray_trimmed[np.ix_(row_mask, col_mask)]

        best_w_row_mid_pxl = best_w_row_mid_pxl - trim_pxl_row[0]
        best_w_col_mid_pxl = best_w_col_mid_pxl - trim_pxl_col[0]

        self.im_gray_trimmed = im_gray_trimmed
        self.im_center = [best_w_row_mid_pxl, best_w_col_mid_pxl]

        self.row_avg_fit = row_avg_fit[(row_pxl > trim_pxl_row[0]) & (row_pxl < trim_pxl_row[1])]
        self.col_avg_fit = col_avg_fit[(col_pxl > trim_pxl_col[0]) & (col_pxl < trim_pxl_col[1])]

    def detect_colonies(self):

        # --- Find the local maxima (= locations of colony centers)
        # Rows
        colony_row_pxl = detect_peaks(self.row_avg_fit, mpd=self.fit_row['period'] / 2, edge='both', kpsh=True, valley=False, show=False)

        # Columns
        colony_col_pxl = detect_peaks(self.col_avg_fit, mpd=self.fit_col['period'] / 2, edge='both', kpsh=True, valley=False, show=False)

        # --- Merge colonies that are very close
        [colony_col_pxl, _] = merge_colonies(colony_col_pxl)
        [colony_row_pxl, _] = merge_colonies(colony_row_pxl)

        # --- Only include the middle 32 rows (just in case more than 32 peaks are detected)
        colony_distance_from_center = self.im_center[0] - colony_row_pxl
        row_right_side = np.sort(-colony_distance_from_center[colony_distance_from_center < 0])[0:16]
        row_left_side = np.sort(colony_distance_from_center[colony_distance_from_center > 0])[0:16]
        colony_row_pxl = np.sort(self.im_center[0] - np.concatenate((row_left_side, -row_right_side), axis=0)).astype(int)

        # --- Only include the middle 48 columns (just in case more than 48 peaks are detected)
        colony_distance_from_center = self.im_center[1] - colony_col_pxl
        col_right_side = np.sort(-colony_distance_from_center[colony_distance_from_center < 0])[0:24]
        col_left_side = np.sort(colony_distance_from_center[colony_distance_from_center > 0])[0:24]
        colony_col_pxl = np.sort(self.im_center[1] - np.concatenate((col_left_side, -col_right_side), axis=0)).astype(int)

        [colony_col_pxl_2d, colony_row_pxl_2d] = np.meshgrid(colony_col_pxl, colony_row_pxl)
        self.colony_pxl = list(zip(colony_row_pxl_2d.ravel(), colony_col_pxl_2d.ravel()))

    def measure_colony_sizes(self):

        # Estimate foreground
        # im_foreground = estimate_foreground_by_gmm(im_gray_trimmed,
        #                                                 colony_row_pxl_2d, colony_col_pxl_2d, n_components=3)
        # im_foreground = estimate_foreground_by_otsu(im_gray_trimmed)
        self.im_foreground = estimate_foreground_by_adaptive_thresholding(self.im_gray_trimmed,
                                                                          block_size=self.fit_row['period'] * 5)

        # Find all objects
        [im_objects, _] = ndi.label(self.im_foreground)

        [colony_row_pxl_2d, colony_col_pxl_2d] = zip(*self.colony_pxl)

        # Only keep objects that correspond to colonies
        colony_objects = im_objects[colony_row_pxl_2d, colony_col_pxl_2d]
        im_objects[~np.isin(im_objects, colony_objects)] = 0

        # Close the small gaps within objects
        im_objects = ndi.binary_closing(im_objects, structure=np.ones((2, 2)), iterations=1)

        # Repeat the labeling
        [im_objects, _] = ndi.label(im_objects)

        # Identify touching colonies (if any) and separate them using watershed segmentation

        # 1. Generate a matrix of colony center markers
        colony_center_coords = np.array(self.colony_pxl).T
        markers = np.zeros(im_objects.shape)
        markers[tuple(colony_center_coords)] = np.arange(colony_center_coords.shape[1])

        # 2. Calculate distance to the background for each pixel in each object
        im_foreground_objects = im_objects > 0
        distance = ndi.distance_transform_edt(im_foreground_objects)

        # 3. Label
        im_objects = watershed(-distance, markers, mask=im_foreground_objects)

        self.im_objects = im_objects

        data = {'col_pxl': colony_col_pxl_2d,
                'row_pxl': colony_row_pxl_2d,
                'label': self.im_objects[colony_row_pxl_2d, colony_col_pxl_2d].astype(int)}

        colony_data = pd.DataFrame(data=data)

        colony_data['col'] = np.digitize(colony_data['col_pxl'].values, colony_data['col_pxl'].unique())
        colony_data['row'] = np.digitize(colony_data['row_pxl'].values, colony_data['row_pxl'].unique())

        # Measure colony size = number of pixels assigned to each colony object
        colony_objects = self.im_objects.ravel().astype(int)
        colony_objects = colony_objects[colony_objects > 0]    # ignore object 0 (= empty spots matching background)
        colony_size = np.bincount(colony_objects)

        colony_data['size'] = colony_size[colony_data['label'].values]
        colony_data['size'] = colony_data['size'].astype(float)

        self.colony_data = colony_data

    def get_colony_data(self):

        return self.colony_data

    def test(self):

        # Test the number of detected rows & columns
        dt = self.colony_data.loc[pd.notnull(self.colony_data['size'])]
        nrows = np.sum(dt.groupby('row')['size'].count() > 24)
        ncols = np.sum(dt.groupby('col')['size'].count() > 16)

        if (nrows < 32) | (ncols < 48):
            if self.verbose:
                print("Warning: One or more rows or columns is possibly missing.")

        # Check for giant colonies & mask them
        md = np.nanmedian(self.colony_data['size'])
        is_giant_colony = self.colony_data['size'] > md*10
        if np.sum(is_giant_colony) > 0:
            if self.verbose:
                print(format('Warning: Masking %d giant colonies.' % np.sum(is_giant_colony)))
        self.colony_data.loc[is_giant_colony, 'size'] = np.nan

    def show_plate(self, ax=None, **kwargs):

        if 'show' in kwargs:
            im = getattr(self, kwargs['show'])
            if kwargs['show'] == 'im_objects':
                colors = np.vstack(([0,0,0], np.random.rand(1536, 3)))
                cmap = matplotlib.colors.ListedColormap(colors)
                kwargs2 = {'cmap': cmap}
            elif kwargs['show'] == 'im_foreground':
                cmap = 'gnuplot'
                kwargs2 = {'cmap': cmap}
            elif kwargs['show'] == 'im':
                kwargs2 = {'cmap': 'gray', 'vmin': 350, 'vmax': 600}
            else:
                cmap = 'gray'
                kwargs2 = {'cmap': cmap}
        else:
            im = self.im_gray_trimmed
            kwargs2 = {'cmap': 'gray', 'vmin': 350, 'vmax': 600}

        if not ax:
            ax = plt.axes()

        if 'row' in kwargs:
            row = kwargs['row']
            col = kwargs['col']

            rows_selected = self.colony_data['row'].isin([row * 2, row * 2 - 1])
            cols_selected = self.colony_data['col'].isin([col * 2, col * 2 - 1])

            y_pxl = self.colony_data.loc[rows_selected & cols_selected, 'row_pxl'].max()
            x_pxl = self.colony_data.loc[rows_selected & cols_selected, 'col_pxl'].min()

            x_pxl = x_pxl - self.fit_col['period'] / 2
            y_pxl = y_pxl + self.fit_row['period'] / 2

            w = self.fit_col['period'] * 2
            h = -self.fit_row['period'] * 2

            x_pxl = np.round(x_pxl - w).astype(int)
            y_pxl = np.round(y_pxl - h).astype(int)

            w = np.round(w * 3).astype(int)
            h = np.round(h * 3).astype(int)

            ax.imshow(im[y_pxl + h:y_pxl, x_pxl:x_pxl + w], **kwargs2)

        else:

            ax.imshow(im, **kwargs2)

        ax.set_xticks([], [])
        ax.set_yticks([], [])

    def show_position(self, row, col, ax=None, **kwargs):
    
        if 'c' in kwargs:
            c = kwargs['c']
        else:
            c = 'r'
    
        rows_selected = self.colony_data['row'].isin([row * 2, row * 2 - 1])
        cols_selected = self.colony_data['col'].isin([col * 2, col * 2 - 1])
    
        y_pxl = self.colony_data.loc[rows_selected & cols_selected, 'row_pxl'].max()
        x_pxl = self.colony_data.loc[rows_selected & cols_selected, 'col_pxl'].min()
    
        x_pxl = x_pxl - self.fit_col['period'] / 2
        y_pxl = y_pxl + self.fit_row['period'] / 2
    
        w = self.fit_col['period'] * 2
        h = -self.fit_row['period'] * 2
    
#         Bottom left x and y coordinates of the square
#         x_pxl = x_pxl-w
#         y_pxl = y_pxl-h
    
#         Width and height of the square
#         w = w*3
#         h = h*3
    
        rect = patches.Rectangle((x_pxl, y_pxl), w, h,
                                 linewidth=2, edgecolor=c, facecolor='none')
    
        if not ax:
            ax = plt.axes()
    
#         ax.imshow(self.im_gray_trimmed)
    
        ax.add_patch(rect)


def merge_colonies(pxl, vals=np.nan, distance_threshold=15):
    distances = np.diff(pxl)
    labels = np.insert(np.cumsum(distances > distance_threshold), 0, 0)
    df = pd.DataFrame(data={'label': labels, 'pxl': pxl, 'val': vals})
    df_merged = df.groupby('label')['pxl', 'val'].mean()

    return df_merged['pxl'].values.astype(int), df_merged['val'].values


def estimate_foreground_by_gmm(im_gray_trimmed, colony_row_pxl_2d, colony_col_pxl_2d, n_components=3):

    # Estimate background vs foreground using a 3-component Guassian Mixed Model
    igt = im_gray_trimmed.reshape(-1, 1)
    gmm = GaussianMixture(n_components=n_components, covariance_type='full').fit(igt)
    cluster_labels = gmm.predict(igt)

    im_cluster_labels = cluster_labels.reshape(im_gray_trimmed.shape)

    h = np.bincount(im_cluster_labels[colony_row_pxl_2d.ravel(), colony_col_pxl_2d.ravel()])
    colony_cluster = np.argmax(h)

    im_foreground = im_cluster_labels == colony_cluster

    return im_foreground


def estimate_foreground_by_otsu(im_gray_trimmed):

    threshold = filters.threshold_otsu(im_gray_trimmed)
    im_foreground = im_gray_trimmed > threshold

    return im_foreground


def estimate_foreground_by_adaptive_thresholding(im_gray_trimmed, block_size=45):

    # Round block size to the nearest odd integer
    block_size = int(np.ceil(block_size) // 2 * 2 + 1)

    adaptive_thresh = filters.threshold_local(im_gray_trimmed, block_size, mode='nearest', method='mean')
    adaptive_thresh = filters.gaussian(adaptive_thresh, sigma=10, mode='nearest')
    im_foreground = im_gray_trimmed > adaptive_thresh

    return im_foreground


def run_platometer(image, save_to_file=False, verbose=True):
    p = Platometer(image['path'], verbose=verbose)

    p.gray_and_trim()
    p.detect_colonies()
    p.measure_colony_sizes()
#     p.test()

    # Saves the object to pickle
    if save_to_file:
        p.save()
    
    return p


def run_platometer_batch(image, save_to_file=False, verbose=False):
    
    plate = run_platometer(image, verbose=verbose)

    colony_data = plate.get_colony_data()
    cols = set(image.keys()) - set(['path'])
    for col in cols:
        colony_data[col] = image[col]

    return colony_data


if __name__ == '__main__':

    start = time.time()

    parser = argparse.ArgumentParser(description='Process plates')
    parser.add_argument('path_to_jpg_list', metavar='path_to_jpg_list', type=str, help='Path to the file containing the list of jpgs to process')

    args = parser.parse_args()
    
    nr_processes = mp.cpu_count()

    folders = pd.read_table(args.path_to_jpg_list, header=None)

    for folder in folders[0]:

        print('Processing %s' % folder)

        # Get path to the processed data folder
        parent_folder = os.path.dirname(args.path_to_jpg_list)
        sub_folder = '/'.join(folder.split('/')[4:])
        data_folder = os.path.join(parent_folder, sub_folder)
        
        if not os.path.exists(data_folder):
            os.makedirs(data_folder)

        # Get list of JPG files in the folder
        jpg_files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f)) and f.lower().endswith('jpg')]
        nr_jpg_files = len(jpg_files)
        jpg_files_id = np.arange(nr_jpg_files)

        # Break the list into smaller chunks of 100 images and process the chunks sequentially
        chunk_size = 100
        chunk_starts = np.arange(0, nr_jpg_files, chunk_size)

        for n_chunk, ix_chunk in enumerate(chunk_starts):

            ix_chunk_start = ix_chunk
            ix_chunk_stop = np.min([ix_chunk+chunk_size-1, nr_jpg_files])
            
            print('Chunk %d, start %d, stop %d' % (n_chunk, ix_chunk_start, ix_chunk_stop))

            this_jpg_files_dict = [{'path': jpg_files[i], 'file_id': jpg_files_id[i]} for i in np.arange(ix_chunk_start, ix_chunk_stop)]
            
            print(len(this_jpg_files_dict))

            pool = mp.Pool(processes=nr_processes)

            chunk_data = pd.DataFrame()

            for res in pool.map_async(run_platometer_batch, this_jpg_files_dict).get():
                if res.shape[0] > 0:
                    chunk_data = chunk_data.append(res, ignore_index=True)

            # Temporarily save the chunk data
            path_to_this_chunk_file = os.path.join(data_folder, format('chunk%d_data.p' % n_chunk))
            save_to_p(chunk_data, path_to_this_chunk_file)

            print('Chunk %d of %d. Execution time: %.2f seconds' % (n_chunk, len(chunk_starts), time.time() - start))

        # Now merge all chunks and delete the temp files
        all_data = pd.DataFrame()

        for n_chunk, ix_chunk in enumerate(chunk_starts):

            path_to_this_chunk_file = os.path.join(data_folder, format('chunk%d_data.p' % n_chunk))
            chunk_data = load(path_to_this_chunk_file)
            all_data = all_data.append(chunk_data, ignore_index=True)

        # Save all data
        path_to_all_data_file = os.path.join(data_folder, 'all_data.p')
        save_to_p(all_data, path_to_all_data_file)
        
        # Save the file_id to path map
        jpg_map = pd.DataFrame(data={'path': jpg_files, 'file_id': jpg_files_id})
        path_to_jpg_map_file = os.path.join(data_folder, 'jpg_map.p')
        save_to_p(jpg_map, path_to_jpg_map_file)


